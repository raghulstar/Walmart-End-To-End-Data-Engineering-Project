{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388d5166-f135-4bcd-8f86-0a370cf65bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/mnt/walmartbronze/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42fb837-a0d1-4be7-80c2-90c6559de7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/mnt/walmartbronze/Tables/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd967e7e-66cf-44ca-98cc-1d9d29e4a41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define raw file paths\n",
    "train_file = \"dbfs:/mnt/walmartbronze/Tables/train/train.csv\"\n",
    "test_file = \"dbfs:/mnt/walmartbronze/Tables/test/test.csv\"\n",
    "stores_file = \"dbfs:/mnt/walmartbronze/Tables/stores/stores.csv\"\n",
    "features_file = \"dbfs:/mnt/walmartbronze/Tables/features/features.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5dea313-05a2-4588-98f7-23ce95c1bf1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Read raw data into DataFrames\n",
    "train_df = spark.read.csv(train_file, header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(test_file, header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(stores_file, header=True, inferSchema=True)\n",
    "features_df = spark.read.csv(features_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98eaf00-6e1b-4580-9556-edb512bdcd8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Perform basic data validation (optional but recommended)\n",
    "print(\"Record Counts:\")\n",
    "print(f\"Train data: {train_df.count()} rows\")\n",
    "print(f\"Test data: {test_df.count()} rows\")\n",
    "print(f\"Stores data: {stores_df.count()} rows\")\n",
    "print(f\"Features data: {features_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50af7db7-120e-4c9d-ad72-3ac5b1df1ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nSchemas:\")\n",
    "train_df.printSchema()\n",
    "test_df.printSchema()\n",
    "stores_df.printSchema()\n",
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96408e7a-bbe9-4b66-a14c-53e620dfad17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "train_df = train_df.dropDuplicates()\n",
    "test_df = test_df.dropDuplicates()\n",
    "stores_df = stores_df.dropDuplicates()\n",
    "features_df = features_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d327d12c-8c09-47b2-b21f-4e83bfc427c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Fill missing values in Features dataset\n",
    "features_df = features_df.fillna({\n",
    "    \"MarkDown1\": 0,\n",
    "    \"MarkDown2\": 0,\n",
    "    \"MarkDown3\": 0,\n",
    "    \"MarkDown4\": 0,\n",
    "    \"MarkDown5\": 0,\n",
    "    \"CPI\": features_df.select(\"CPI\").dropna().agg({\"CPI\": \"avg\"}).collect()[0][0],\n",
    "    \"Unemployment\": features_df.select(\"Unemployment\").dropna().agg({\"Unemployment\": \"avg\"}).collect()[0][0]\n",
    "})\n",
    "\n",
    "# Handle missing `Size` in Stores dataset\n",
    "stores_df = stores_df.fillna({\"Size\": stores_df.select(\"Size\").dropna().agg({\"Size\": \"median\"}).collect()[0][0]})\n",
    "\n",
    "# Handle missing sales in train dataset\n",
    "train_df = train_df.na.fill({\"Weekly_Sales\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786dd829-5024-481c-970a-78ddc91fb10d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, mean, stddev\n",
    "# -- Train Dataset Cleaning --\n",
    "# Remove negative sales (invalid data)\n",
    "train_df = train_df.filter(col(\"Weekly_Sales\") >= 0)\n",
    "\n",
    "# Handle outliers in Weekly_Sales using Z-Score\n",
    "sales_stats = train_df.select(mean(\"Weekly_Sales\").alias(\"mean\"), stddev(\"Weekly_Sales\").alias(\"std\")).collect()[0]\n",
    "mean_sales, std_sales = sales_stats[\"mean\"], sales_stats[\"std\"]\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"z_score\",\n",
    "    (col(\"Weekly_Sales\") - mean_sales) / std_sales\n",
    ").filter(col(\"z_score\").between(-3, 3)).drop(\"z_score\")\n",
    "\n",
    "# Convert Date column to a standard format\n",
    "train_df = train_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "# -- Test Dataset Cleaning --\n",
    "# Convert Date column to a standard format\n",
    "test_df = test_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a25dbb-ba62-4f2a-ae47-78a2df2d1341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, upper, trim\n",
    "\n",
    "# -- Stores Dataset Cleaning --\n",
    "\n",
    "# Standardize store types (e.g., trim whitespace or fix inconsistent case)\n",
    "stores_df = stores_df.withColumn(\"Type\", trim(upper(col(\"Type\"))))  # Use trim and upper from PySpark functions\n",
    "\n",
    "# Handle outliers in Size (e.g., replace extreme sizes with median)\n",
    "size_stats = stores_df.select(mean(\"Size\").alias(\"mean\"), stddev(\"Size\").alias(\"std\")).collect()[0]\n",
    "median_size = stores_df.approxQuantile(\"Size\", [0.5], 0)[0]\n",
    "\n",
    "stores_df = stores_df.withColumn(\n",
    "    \"Size\",\n",
    "    when(col(\"Size\") < 0, median_size).otherwise(col(\"Size\"))\n",
    ")\n",
    "\n",
    "# -- Features Dataset Cleaning --\n",
    "\n",
    "# Convert Date column to a standard format\n",
    "features_df = features_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "# Handle outliers in numerical columns (MarkDown1-5, CPI, Unemployment)\n",
    "numerical_columns = [\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"CPI\", \"Unemployment\"]\n",
    "\n",
    "for col_name in numerical_columns:\n",
    "    stats = features_df.select(mean(col_name).alias(\"mean\"), stddev(col_name).alias(\"std\")).collect()[0]\n",
    "    mean_value, std_value = stats[\"mean\"], stats[\"std\"]\n",
    "    \n",
    "    features_df = features_df.withColumn(\n",
    "        col_name,\n",
    "        when(\n",
    "            col(col_name).between(mean_value - 3 * std_value, mean_value + 3 * std_value), col(col_name)\n",
    "        ).otherwise(mean_value)\n",
    "    )\n",
    "\n",
    "# Fill missing MarkDown values with 0 (if still missing after outlier handling)\n",
    "features_df = features_df.fillna({col_name: 0 for col_name in numerical_columns})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdd0fb6c-c5a7-4e5f-a932-0bae14e68bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Writing All DF to Silver Container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12398fb5-3fa4-4601-b74d-4af338a8713b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/mnt/walmartsilver/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074f4724-175f-46b7-82ec-e85e4e68ad84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/mnt/walmartsilver/\"\n",
    "\n",
    "# Function to overwrite Delta files cleanly\n",
    "def write_clean_delta(df, folder_name):\n",
    "    path = f\"{base_path}{folder_name}\"\n",
    "    # Remove the contents of the directory to ensure a clean overwrite\n",
    "    files = dbutils.fs.ls(path)\n",
    "    for file in files:\n",
    "        dbutils.fs.rm(file.path, True)\n",
    "    # Write the DataFrame in Delta format\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "\n",
    "# Write each DataFrame to its respective folder\n",
    "write_clean_delta(features_df, \"Features Silver\")\n",
    "write_clean_delta(train_df, \"Train Silver\")\n",
    "write_clean_delta(test_df, \"Test Silver\")\n",
    "write_clean_delta(stores_df, \"Stores Silver\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}