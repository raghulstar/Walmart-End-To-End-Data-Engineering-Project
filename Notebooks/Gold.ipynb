{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972cedf7-6983-4dc9-b5dd-63827e713705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import col, mean, sum, desc\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# ---------------------------------\n",
    "# Step 1: Load Silver Layer Data\n",
    "# ---------------------------------\n",
    "train_df = spark.read.format('delta')\\\n",
    "                    .option('inferSchema',True)\\\n",
    "                    .option('header',True)\\\n",
    "                    .load('dbfs:/mnt/walmartsilver/Final Silver/Train Silver/')\n",
    "test_df = spark.read.format('delta')\\\n",
    "                    .option('inferSchema',True)\\\n",
    "                    .option('header',True)\\\n",
    "                    .load('dbfs:/mnt/walmartsilver/Final Silver/Test Silver/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd9f464-8743-4a3d-968e-5a5d0aaddf09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cast columns to Double\n",
    "columns_to_cast = [\n",
    "    \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \n",
    "    \"CPI\", \"Unemployment\", \"Temperature\", \"Fuel_Price\", \"Size\"\n",
    "]\n",
    "\n",
    "for column_name in columns_to_cast:\n",
    "    train_df = train_df.withColumn(column_name, col(column_name).cast(\"double\"))\n",
    "    test_df = test_df.withColumn(column_name, col(column_name).cast(\"double\"))\n",
    "\n",
    "# Verify the data types\n",
    "print(\"Training Data Schema:\")\n",
    "train_df.printSchema()\n",
    "\n",
    "print(\"Testing Data Schema:\")\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb61e0b2-d4de-4736-b95d-af0b5924bab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Step 2: Prepare Data for Modeling\n",
    "# ---------------------------------\n",
    "# Feature selection: Columns used for the regression model\n",
    "feature_columns = [\n",
    "    \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \n",
    "    \"MarkDown4\", \"MarkDown5\", \"CPI\", \"Unemployment\", \"Size\", \"IsHoliday\"\n",
    "]\n",
    "\n",
    "# Use VectorAssembler to combine feature columns into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Prepare training data\n",
    "train_data = assembler.transform(train_df).select(\"features\", \"Weekly_Sales\")\n",
    "train_data.show(5)\n",
    "\n",
    "# Prepare test data\n",
    "test_data = assembler.transform(test_df).select(\"features\", \"Store\", \"Dept\", \"Date\", \"IsHoliday\")\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4415a5-19ce-440d-a5e4-21e3c2eac618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Step 3: Train the Regression Model\n",
    "# ---------------------------------\n",
    "# Initialize and train the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Weekly_Sales\", predictionCol=\"prediction\")\n",
    "\n",
    "# Fit the model\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Model evaluation on training data\n",
    "training_summary = lr_model.summary\n",
    "print(\"Model Training Summary:\")\n",
    "print(f\"RMSE: {training_summary.rootMeanSquaredError}\")\n",
    "print(f\"R2: {training_summary.r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f563ce9f-d7c6-486b-932f-94a01d069da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ---------------------------------\n",
    "# Split train data into training and validation sets\n",
    "# ---------------------------------\n",
    "train_subset, validation_subset = train_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Weekly_Sales\", numTrees=50, maxDepth=10)\n",
    "\n",
    "# Train the model on train subset\n",
    "rf_model = rf.fit(train_subset)\n",
    "\n",
    "# Predict on validation set\n",
    "validation_predictions = rf_model.transform(validation_subset)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"Weekly_Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Weekly_Sales\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(validation_predictions)\n",
    "r2 = evaluator_r2.evaluate(validation_predictions)\n",
    "\n",
    "print(f\"Random Forest Model Evaluation on Validation Set:\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"RÂ²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b6b3a9-7e54-4886-8004-028229f8acd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Predict on test dataset (without Weekly_Sales)\n",
    "# ---------------------------------\n",
    "test_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Select required columns for final output\n",
    "final_predictions = test_predictions.select(\"Store\", \"Dept\", \"Date\", \"IsHoliday\", col(\"prediction\").alias(\"Predicted_Weekly_Sales\"))\n",
    "\n",
    "# Show test predictions\n",
    "final_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d447091f-a4a1-4bf1-8c43-21482b861e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Simulate Current Stock Levels\n",
    "# ---------------------------\n",
    "# Assume a simulated current stock level for each store and department\n",
    "# For demonstration, let's assign an arbitrary initial stock of 500 units for each department\n",
    "current_stock_df = final_predictions.withColumn(\"Current_Stock\", lit(500))\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Calculate Remaining Stock and Reorder Flag\n",
    "# ---------------------------\n",
    "# Subtract predicted weekly sales from current stock to estimate remaining stock\n",
    "stock_analysis_df = current_stock_df.withColumn(\n",
    "    \"Remaining_Stock\", col(\"Current_Stock\") - col(\"Predicted_Weekly_Sales\")\n",
    ")\n",
    "\n",
    "# Flag departments where stock is below a reorder threshold (e.g., 100 units)\n",
    "stock_analysis_df = stock_analysis_df.withColumn(\n",
    "    \"Reorder_Flag\", when(col(\"Remaining_Stock\") < 100, \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Save Final Stock Analysis to Gold Layer\n",
    "# ---------------------------\n",
    "# Select relevant columns for output\n",
    "final_stock_analysis = stock_analysis_df.select(\n",
    "    \"Store\", \"Dept\", \"Date\", \"IsHoliday\", \"Predicted_Weekly_Sales\", \"Current_Stock\", \n",
    "    \"Remaining_Stock\", \"Reorder_Flag\"\n",
    ")\n",
    "\n",
    "# Show stock analysis\n",
    "final_stock_analysis.show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3faa4ed3-3b27-4838-9111-6d7e003fdbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/mnt/walmartgold/\"\n",
    "\n",
    "# Function to overwrite Delta files cleanly\n",
    "def write_clean_delta(df, folder_name):\n",
    "    path = f\"{base_path}{folder_name}\"\n",
    "    # Remove the contents of the directory to ensure a clean overwrite\n",
    "    files = dbutils.fs.ls(path)\n",
    "    for file in files:\n",
    "        dbutils.fs.rm(file.path, True)\n",
    "    # Write the DataFrame in Delta format\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "\n",
    "# Write each DataFrame to its respective folder\n",
    "write_clean_delta(final_predictions, \"Predicted Sales\")\n",
    "write_clean_delta(final_stock_analysis, \"Stock Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e848618-4ee4-416a-ac46-cb35cd198725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame as a Delta table\n",
    "final_stock_analysis.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"final_stock_analysis\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}